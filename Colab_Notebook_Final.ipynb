{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/AksharJ47/regression-predict-api-template/blob/master/Colab_Notebook_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kSNs9C46Kaup"
   },
   "source": [
    "# Import Packages and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "colab_type": "code",
    "id": "LSI_ooLcKauq",
    "outputId": "660c6a9b-e436-42b0-af37-4dc1d66f6fa8"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nDT_p0OjKauw"
   },
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XGek2kQRKaux"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('https://raw.githubusercontent.com/AksharJ47/regression-predict-api-template/master/utils/data/Train_Zindi.csv')\n",
    "test_df = pd.read_csv('https://raw.githubusercontent.com/AksharJ47/regression-predict-api-template/master/utils/data/Test_Zindi.csv')\n",
    "riders_df = pd.read_csv('https://raw.githubusercontent.com/AksharJ47/regression-predict-api-template/master/utils/data/Riders_Zindi.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3plVgnspKau1"
   },
   "outputs": [],
   "source": [
    "# Find datatypes and missing values\n",
    "# Drop data not available in test, Pickup Time + label = Arrival times\n",
    "train_df = train_df.drop(['Arrival at Destination - Day of Month',\n",
    "                          'Arrival at Destination - Weekday (Mo = 1)',\n",
    "                          'Arrival at Destination - Time'], axis=1)\n",
    "test_df['Time from Pickup to Arrival'] = [np.nan]*test_df.shape[0]\n",
    "full_df = pd.concat([train_df, test_df], axis=0, ignore_index=True)\n",
    "\n",
    "merged_df = pd.merge(full_df, riders_df, how='left',\n",
    "                     left_on='Rider Id',\n",
    "                     right_on='Rider Id',\n",
    "                     left_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "colab_type": "code",
    "id": "m4TYHsmSKau6",
    "outputId": "ea03adb6-a2f1-4e88-8e14-2871d37edc01"
   },
   "outputs": [],
   "source": [
    "# view dataset\n",
    "merged_df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tlRpOrIiKavA",
    "outputId": "a4854f29-2188-4707-ef90-6008a2c3648f"
   },
   "outputs": [],
   "source": [
    "# Find datatypes and missing values\n",
    "merged_df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cK-ZIiEfKavF"
   },
   "source": [
    "# Data Cleaning and Formating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "colab_type": "code",
    "id": "Nv88lnxgKavG",
    "outputId": "4cc57873-0f74-4ccc-c117-2491f3704747"
   },
   "outputs": [],
   "source": [
    "feature_names = {\"Order No\": \"Order_No\",\n",
    "                 \"User Id\": \"User_Id\",\n",
    "                 \"Vehicle Type\": \"Vehicle_Type\",\n",
    "                 \"Personal or Business\": \"Personal_Business\",\n",
    "                 \"Placement - Day of Month\": \"Pla_Mon\",\n",
    "                 \"Placement - Weekday (Mo = 1)\": \"Pla_Weekday\",\n",
    "                 \"Placement - Time\": \"Pla_Time\",\n",
    "                 \"Confirmation - Day of Month\": \"Con_Day_Mon\",\n",
    "                 \"Confirmation - Weekday (Mo = 1)\": \"Con_Weekday\",\n",
    "                 \"Confirmation - Time\": \"Con_Time\",\n",
    "                 \"Arrival at Pickup - Day of Month\": \"Arr_Pic_Mon\",\n",
    "                 \"Arrival at Pickup - Weekday (Mo = 1)\": \"Arr_Pic_Weekday\",\n",
    "                 \"Arrival at Pickup - Time\": \"Arr_Pic_Time\",\n",
    "                 \"Platform Type\": \"Platform_Type\",\n",
    "                 \"Pickup - Day of Month\": \"Pickup_Mon\",\n",
    "                 \"Pickup - Weekday (Mo = 1)\": \"Pickup_Weekday\",\n",
    "                 \"Pickup - Time\": \"Pickup_Time\",\n",
    "                 \"Distance (KM)\": \"Distance(km)\",\n",
    "                 \"Precipitation in millimeters\": \"Precipitation(mm)\",\n",
    "                 \"Pickup Lat\": \"Pickup_Lat\",\n",
    "                 \"Pickup Long\": \"Pickup_Lon\",\n",
    "                 \"Destination Lat\": \"Destination_Lat\",\n",
    "                 \"Destination Long\": \"Destination_Lon\",\n",
    "                 \"Rider Id\": \"Rider_Id\",\n",
    "                 \"Time from Pickup to Arrival\": \"Time_Pic_Arr\"}\n",
    "\n",
    "renamed_df = merged_df.rename(columns=feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "renamed_df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IRtOtp_mKavL"
   },
   "outputs": [],
   "source": [
    "# Assume 0 Millimetres of rain where precipitation is missing\n",
    "# Impute missing temperature with the average\n",
    "\n",
    "\n",
    "def Impute(input_df):\n",
    "    '''Function fills missing values on the Temperature and\n",
    "       Precipitation columns, all missing temperatures are\n",
    "       imputed with the average temperature while all\n",
    "       Precipitation columns are filled with 0mm of rain\n",
    "    '''\n",
    "    df = input_df.copy()\n",
    "    cols_to_impute = ['Temperature',\n",
    "                      'Precipitation(mm)']\n",
    "    for col in cols_to_impute:\n",
    "        if col == 'Temperature':\n",
    "            a = round(df[col].mean(), 1)\n",
    "        if col == 'Precipitation(mm)':\n",
    "            a = round(df[col].mean(), 1)\n",
    "        df[col] = df[col].fillna(a)\n",
    "    return (df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JaSSvRzFKavR"
   },
   "outputs": [],
   "source": [
    "imputed_df = Impute(renamed_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kHfvOf3BKavW"
   },
   "outputs": [],
   "source": [
    "def time_change(input_df):\n",
    "    '''Converts time format %H:%M:%S to seconds past midnight(00:00) of\n",
    "       the same day rounded to the nearest second.\n",
    "       ------------------------------\n",
    "       12:00:00 PM --> 43200\n",
    "       01:30:00 AM --> 5400\n",
    "       02:35:30 PM --> 9330\n",
    "     '''\n",
    "    df = input_df.copy()\n",
    "    from pandas.api.types import is_numeric_dtype\n",
    "\n",
    "    def time_fn(row):\n",
    "        b = row.split(' ')\n",
    "        if b[1] == 'AM':\n",
    "            c = 0\n",
    "        else:\n",
    "            c = 12\n",
    "        b = b[0].split(':')\n",
    "        b = [int(i) for i in b]\n",
    "        if b[0] == 12:\n",
    "            c -= 12\n",
    "        # convertion to hours\n",
    "        b[0] = (b[0] + c)*3600\n",
    "        b[1] = (b[1])*60.0\n",
    "        b[2] = (b[2])\n",
    "        row = int(sum(b))\n",
    "        return(row)\n",
    "    time_columns = [\n",
    "                'Pla_Time',\n",
    "                'Con_Time',\n",
    "                'Arr_Pic_Time',\n",
    "                'Pickup_Time',\n",
    "               ]\n",
    "    for col in df.columns:\n",
    "        if col in time_columns:\n",
    "            if is_numeric_dtype(df[col]) is False:\n",
    "                df[col] = df[col].apply(lambda x: time_fn(x))\n",
    "            else:\n",
    "                pass\n",
    "    return(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TOU9rw6DKava"
   },
   "outputs": [],
   "source": [
    "time_changed_df = time_change(imputed_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "colab_type": "code",
    "id": "IolTHmdLKave",
    "outputId": "c382c926-46fc-4cd4-b190-df42e75cfc77"
   },
   "outputs": [],
   "source": [
    "time_changed_df.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2RFzUvSpKavj"
   },
   "source": [
    "# Model Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R2oQ1iwvKavn"
   },
   "source": [
    "**New columns added to the dataset**\n",
    "1. Rider Experience (Low,Medium,High):\n",
    "   * Age < 25th percentile --> **Low Experience**\n",
    "   * 25th percentile < Age < 75th percentile --> **Medium experience**\n",
    "   * Age > 75th percentile --> **High experience**\n",
    "2. Temp_Band (Low,Medium,High)\n",
    "   * Temp. < 25th percentile --> **Low Temperature**\n",
    "   * 25th percentile < T < 75th percentile --> **Medium Temperature**\n",
    "   * T > 75th percentile --> **High Temperature**\n",
    "3. Conf_Pla_dif\n",
    "   The time duration between placement and confirmation in seconds\n",
    "4. Arr_Con_dif\n",
    "   Time difference between arrival and confirmation time in seconds\n",
    "5. Pic_Arr_dif\n",
    "   Time difference between pickup and package arrival in seconds\n",
    "6. Manhattand and harvesine distance between the pickup and\n",
    "   destination points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4fMUsbpsKavn"
   },
   "outputs": [],
   "source": [
    "# Add ride experience column\n",
    "time_changed_df['Rider_Exp'] = pd.qcut(time_changed_df['Age'],\n",
    "                                       q=[0, .25, .75, 1],\n",
    "                                       labels=['low', 'medium', 'high'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9VfbKv_WKavs"
   },
   "outputs": [],
   "source": [
    "# Create temperature band column - 3 categories - low, mid, high\n",
    "time_changed_df['Temp_Band'] = pd.qcut(time_changed_df['Temperature'],\n",
    "                                       q=[0, .25, .75, 1],\n",
    "                                       labels=['low', 'medium', 'high'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zp-5dJcmKavv"
   },
   "outputs": [],
   "source": [
    "def time_diffs(input_df):\n",
    "    '''Inputs news columns of time diffrence between the following columns\n",
    "       1. order confimartion and placement time\n",
    "       2. order arrival and confirmation time\n",
    "       3. order pickup and arrival time\n",
    "    '''\n",
    "    df = input_df.copy()\n",
    "    df['Conf_Pla_dif'] = df['Con_Time'] - df['Pla_Time']\n",
    "    df['Arr_Con_dif'] = df['Arr_Pic_Time'] - df['Con_Time']\n",
    "    df['Pic_Arr_dif'] = df['Pickup_Time'] - df['Arr_Pic_Time']\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_changed_df = time_diffs(time_changed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PuL9QYynKavz"
   },
   "outputs": [],
   "source": [
    "# Create manhattan distance\n",
    "def manhattan(input_df):\n",
    "    '''Calculates the manhattan distance between two location given\n",
    "       the longitude and latitude of the locations\n",
    "    '''\n",
    "    df = input_df.copy()\n",
    "    a = np.abs(df['Pickup_Lat'] - df['Destination_Lat'])\n",
    "    b = np.abs(df['Pickup_Lon'] - df['Destination_Lon'])\n",
    "    df['manhattan_dist'] = a + b\n",
    "    return (df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8LtNUQEdKav2"
   },
   "outputs": [],
   "source": [
    "manhattan_df = manhattan(time_changed_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J5rJyVPmKav9"
   },
   "outputs": [],
   "source": [
    "def haversine_array(lat1, lng1, lat2, lng2):\n",
    "    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2))\n",
    "    AVG_EARTH_RADIUS = 6371  # in km\n",
    "    lat = lat2 - lat1\n",
    "    lng = lng2 - lng1\n",
    "    d = (np.sin(lat * 0.5) ** 2 +\n",
    "         np.cos(lat1) * np.cos(lat2) *\n",
    "         np.sin(lng * 0.5) ** 2)\n",
    "    h = 2 * AVG_EARTH_RADIUS * np.arcsin(np.sqrt(d))\n",
    "    return h\n",
    "\n",
    "\n",
    "def add_haversine(input_df):\n",
    "    input_df_1 = input_df.copy()\n",
    "    input_df_1['distance_haversine'] = haversine_array(\n",
    "        input_df_1['Pickup_Lat'].values,\n",
    "        input_df_1['Pickup_Lon'].values,\n",
    "        input_df_1['Destination_Lat'].values,\n",
    "        input_df_1['Destination_Lon'].values)\n",
    "    return input_df_1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "colab_type": "code",
    "id": "OEpd6YgkKawA",
    "outputId": "7ae36a58-607c-4c0a-aa3f-6dc319c462d1"
   },
   "outputs": [],
   "source": [
    "harvestine_df = add_haversine(manhattan_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = (harvestine_df[harvestine_df['Pla_Mon'] !=\n",
    "                         harvestine_df['Pickup_Mon']].index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "snHS4UO4KawE"
   },
   "outputs": [],
   "source": [
    "# Encode Rider Exp,Temp_Band and Personal/Business\n",
    "def encode_normalize(input_df):\n",
    "    from pandas.api.types import is_numeric_dtype\n",
    "    df = input_df.copy()\n",
    "    to_encode = ['Rider_Exp',\n",
    "                 'Personal_Business',\n",
    "                 'Temp_Band']\n",
    "    for col in (df.drop(to_encode, axis=1).columns):\n",
    "        if is_numeric_dtype(df[col]) and col not in to_encode and col != \"Time_Pic_Arr\":\n",
    "            from sklearn.preprocessing import StandardScaler\n",
    "            scaler = StandardScaler()\n",
    "            df[[col]] = scaler.fit_transform(df[[col]])\n",
    "    df = pd.get_dummies(df, columns=to_encode, drop_first=True)\n",
    "    return(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_df = encode_normalize(harvestine_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hh0h5qW_KawI"
   },
   "outputs": [],
   "source": [
    "# Extract feature columns\n",
    "numeric_cols = []\n",
    "object_cols = []\n",
    "time_cols = []\n",
    "for k, v in normal_df.dtypes.items():\n",
    "    if (v != object):\n",
    "        if (k != \"Time_Pic_Arr\"):\n",
    "            numeric_cols.append(k)\n",
    "    elif k.endswith(\"Time\"):\n",
    "        time_cols.append(k)\n",
    "    else:\n",
    "        object_cols.append(k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xKXfkxjoKawL"
   },
   "outputs": [],
   "source": [
    "# data_df = data_encoded_df[numeric_cols]\n",
    "y = normal_df[:len(train_df)][['Time_Pic_Arr']]\n",
    "X = normal_df[numeric_cols][:len(train_df)]\n",
    "test = normal_df[numeric_cols][len(train_df):]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DW27roZuKawP"
   },
   "source": [
    "# Model Building and Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FXAEjs8gKawP"
   },
   "outputs": [],
   "source": [
    "# Split data into training and validation sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    shuffle=True,\n",
    "                                                    random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m-8iRREPKawU"
   },
   "source": [
    "## Model Selection\n",
    "The data was first validated using 6 Regression models to analyse which model potentially fits the data best, kfold cross validation was used with the data split into 10 different test and validation sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jl2hsLjXKawU"
   },
   "source": [
    "**Import and load models** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CouhvLA0KawW"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import (ExtraTreesRegressor,\n",
    "                              RandomForestRegressor,\n",
    "                              GradientBoostingRegressor)\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "colab_type": "code",
    "id": "5vYig26NKawa",
    "outputId": "19a47e4b-a91c-4369-bdae-13ee6957752a"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from sklearn.model_selection import (KFold,\n",
    "                                     GridSearchCV,\n",
    "                                     RandomizedSearchCV)\n",
    "from sklearn.model_selection import (cross_val_score,\n",
    "                                     learning_curve)\n",
    "rs = 42\n",
    "kfold = KFold(n_splits=10, random_state=rs, shuffle=True)\n",
    "\n",
    "regressors = []\n",
    "regressors.append(SVR(gamma='scale'))\n",
    "regressors.append(GradientBoostingRegressor(random_state=rs))\n",
    "regressors.append(ExtraTreesRegressor(n_estimators=rs))\n",
    "regressors.append(RandomForestRegressor(random_state=rs,\n",
    "                                        n_estimators=100))\n",
    "regressors.append(xgb.XGBRegressor(random_state=rs,\n",
    "                                   objective='reg:squarederror'))\n",
    "regressors.append(lgb.LGBMRegressor(random_state=rs))\n",
    "\n",
    "cv_results = []\n",
    "rmse_scorer = make_scorer(mean_squared_error)\n",
    "for regressor in regressors:     # scores to be minimised are negated (neg)\n",
    "    cv_results.append(np.sqrt(cross_val_score(estimator=regressor,\n",
    "                                              X=X_train,\n",
    "                                              y=y_train,\n",
    "                                              cv=kfold,\n",
    "                                              scoring=rmse_scorer)))\n",
    "cv_means = []\n",
    "cv_stds = []\n",
    "for cv_result in cv_results:\n",
    "    cv_means.append(cv_result.mean())\n",
    "    cv_stds.append(cv_result.std())\n",
    "\n",
    "cv_res = pd.DataFrame({\n",
    "    'Algorithm': ['LGBM', 'SVR', 'GBR', 'EXR', 'RFR', 'XGBR', 'LGBM'],\n",
    "    'CrossValMeans': cv_means, 'CrossValErrors': cv_stds\n",
    "                       })\n",
    "cv_res = cv_res.sort_values('CrossValMeans', ascending=True)\n",
    "print(cv_res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O1M_Qb28Kawe"
   },
   "source": [
    "## Hyperparemeter tuning\n",
    "From the above validation it can be seen that the following models produces the lowest errors:\n",
    "1. LGBM\n",
    "2. GBR\n",
    "3. XGBR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "colab_type": "code",
    "id": "C3Bc9bzLKawf",
    "outputId": "6818273c-6ac4-4e3b-9785-f1e9223d727e"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "A Grid search is done to determine the optimal parameters\n",
    "for the LGBMRegressor\n",
    "\n",
    "The search range is defined below :-\n",
    "\n",
    "    'n_estimators': [75, 95],\n",
    "    'num_leaves'  : [12,15, 17],\n",
    "    'reg_alpha'   : [0.02, 0.05],\n",
    "    'min_data_in_leaf' : [250, 280, 300]\n",
    "    'learning_rate': [0.05, 0.1, 0.25],\n",
    "    'objective'    : ['regression', None]\n",
    "\n",
    "'''\n",
    "\n",
    "params = {\n",
    "    'n_estimators': [75],\n",
    "    'num_leaves': [15],\n",
    "    'reg_alpha': [0.02],\n",
    "    'min_data_in_leaf': [300],\n",
    "    'learning_rate': [0.1],\n",
    "    'objective': ['regression']\n",
    "    }\n",
    "\n",
    "lsearch = GridSearchCV(estimator=lgb.LGBMRegressor(random_state=rs),\n",
    "                       cv=kfold, scoring=rmse_scorer, param_grid=params)\n",
    "lgbm = lsearch.fit(X_train, y_train)\n",
    "\n",
    "l_params = lgbm.best_params_\n",
    "l_score = np.sqrt(abs(lgbm.best_score_))\n",
    "print(lgbm.best_params_, np.sqrt(abs(lgbm.best_score_)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random search CV\n",
    "param = {'boosting_type': ['gbdt'],\n",
    "         'learning_rate': [0.05, 0.1, 0.15],\n",
    "         'max_depth': [-1],\n",
    "         'n_estimators': [75, 100, 150],\n",
    "         'n_jobs': [-1],\n",
    "         'num_leaves': [12, 15, 17],\n",
    "         'objective': ['Regression'],\n",
    "         'random_state': [42],\n",
    "         'bagging_fraction': [0.1, 0.5, 0.7, 0.9]}\n",
    "rs_lgbm = RandomizedSearchCV(lgb.LGBMRegressor(),\n",
    "                             cv=kfold,\n",
    "                             scoring=rmse_scorer,\n",
    "                             n_iter=10,\n",
    "                             param_distributions=param)\n",
    "rs_mod = rs_lgbm.fit(X, y)\n",
    "rs_params = rs_mod.best_params_\n",
    "rs_score = np.sqrt(abs(rs_mod.best_score_))\n",
    "for i in rs_params.keys():\n",
    "    print(str(i) + ': ' + str(rs_params[i]))\n",
    "print('lowest RMSE: ' + str(rs_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "colab_type": "code",
    "id": "M6u03K7AKawi",
    "outputId": "fce43343-2b2a-4cc6-e43c-e6333b5d0eab"
   },
   "outputs": [],
   "source": [
    "RFC = RandomForestRegressor(random_state=rs)\n",
    "rf_param = {'max_depth': [None],\n",
    "            'max_features': [3],\n",
    "            'min_samples_split': [10],\n",
    "            'min_samples_leaf': [3],\n",
    "            'n_estimators': [300]}\n",
    "rsearch = GridSearchCV(RFC, cv=kfold,\n",
    "                       scoring=rmse_scorer,\n",
    "                       param_grid=rf_param)\n",
    "rfm = rsearch.fit(X_train, y_train)\n",
    "\n",
    "r_score = np.sqrt(abs(rfm.best_score_))\n",
    "r_params = rfm.best_params_\n",
    "print(r_score, r_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P_9sF1WiKawl"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def plot_learning_curve(estimator, title,\n",
    "                        X, y, ylim=None, n_jobs=-1,\n",
    "                        cv=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    \"\"\"Generating a plot of test and training learning curve\"\"\"\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"RMSE\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes,\n",
    "        scoring=rmse_scorer, shuffle=True)\n",
    "\n",
    "    # scores - 5 runs, each with 10 fold\n",
    "    train_scores_mean = np.mean(np.sqrt(train_scores), axis=1)\n",
    "    train_scores_std = np.std(np.sqrt(train_scores), axis=1)\n",
    "    test_scores_mean = np.mean(np.sqrt(test_scores), axis=1)\n",
    "    test_scores_std = np.std(np.sqrt(test_scores), axis=1)\n",
    "\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std,\n",
    "                     alpha=0.1, color='r')\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std,\n",
    "                     alpha=0.1, color='g')\n",
    "\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color='r',\n",
    "             label='Training score')\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color='g',\n",
    "             label='Cross-validation score')\n",
    "\n",
    "    plt.legend(loc='best')\n",
    "    return plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "colab_type": "code",
    "id": "pxrzv3_jKawp",
    "outputId": "ff03342f-85cf-43e1-a2ca-e2d0f993f4d0"
   },
   "outputs": [],
   "source": [
    "# Learning Curves\n",
    "g = plot_learning_curve(lgbm.best_estimator_, 'LGBM Learning Curves',\n",
    "                        X_train, y_train, cv=kfold)\n",
    "plt.title('LGBM Learning Curves', fontsize=18)\n",
    "g = plot_learning_curve(rfm.best_estimator_, 'RFR Learning Curves',\n",
    "                        X_train, y_train, cv=kfold)\n",
    "plt.title('RFR Learning Curves', fontsize=18)\n",
    "# lgbm: mse error comment here\n",
    "# rf: mse error comment here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "colab_type": "code",
    "id": "ZbcT2E1dKawt",
    "outputId": "83a45bd5-e0a7-4593-8234-61b2d32a4c41"
   },
   "outputs": [],
   "source": [
    "vals = lgbm.best_estimator_.feature_importances_\n",
    "l_importance = np.array([val/sum(vals) for val in vals])\n",
    "r_importance = rfm.best_estimator_.feature_importances_\n",
    "feats = X_train.columns\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 10))\n",
    "plt.subplots_adjust(top=0.6, bottom=0.2, hspace=.6, wspace=0.8)\n",
    "\n",
    "indices = np.argsort(l_importance)[::-1]\n",
    "g = sns.barplot(y=feats[indices], x=l_importance[indices],\n",
    "                orient='h', ax=axes[0])\n",
    "g.set_xlabel('Relative importances', fontsize=12)\n",
    "g.set_ylabel('Features', fontsize=12)\n",
    "g.tick_params(labelsize=9.3)\n",
    "g.set_title('LGBM feature importance', fontsize=18)\n",
    "\n",
    "index = np.argsort(r_importance)[::-1]\n",
    "g = sns.barplot(y=feats[index], x=r_importance[index],\n",
    "                orient='h', ax=axes[1])\n",
    "g.set_xlabel('Relative importances', fontsize=12)\n",
    "g.set_ylabel('Features', fontsize=12)\n",
    "g.tick_params(labelsize=9.3)\n",
    "g.set_title('Random Forest feature importance', fontsize=18)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HpR92A3BKawy"
   },
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "colab_type": "code",
    "id": "kHydG7jyKawy",
    "outputId": "067ce9f8-d265-4e8e-8258-011659a57fa0"
   },
   "outputs": [],
   "source": [
    "lgb_train = lgb.Dataset(X_train, y_train)\n",
    "lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)\n",
    "\n",
    "lparams = {'learning_rate': 0.1,\n",
    "           'min_data_in_leaf': 300, \n",
    "           'n_estimators': 75,\n",
    "           'num_leaves': 20,\n",
    "           'random_state':rs,\n",
    "           'objective': 'regression',\n",
    "           'reg_alpha': 0.02,\n",
    "           'feature_fraction': 0.9,\n",
    "           'bagging_fraction':0.9}\n",
    "\n",
    "\n",
    "lgbm = lgb.train(lparams, lgb_train, valid_sets=lgb_eval, num_boost_round=20,\n",
    "                 early_stopping_rounds=20)\n",
    "\n",
    "lpred = lgbm.predict(X_test, num_iteration=lgbm.best_iteration)\n",
    "\n",
    "print(\"The RMSE of prediction is \", mean_squared_error(y_test, lpred)**0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BYWkHe13Kaw1"
   },
   "source": [
    "# Data Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "colab_type": "code",
    "id": "ND2n6t-iKaw2",
    "outputId": "cb919597-3bea-49db-eaa3-4b2176044ebc"
   },
   "outputs": [],
   "source": [
    "plt.hist2d(x = time_changed_df['Pla_Mon'],y = time_changed_df['Pla_Time']/3600,\n",
    "           bins=(10,20),\n",
    "           range=((0,31),(0,24)))\n",
    "plt.xlabel('Day_of_placement')\n",
    "plt.ylabel('Hour_of_placement')\n",
    "plt.title('service hours distribution')\n",
    "plt.xticks(np.arange(0, 31, step=2))\n",
    "plt.yticks(np.arange(0, 24, step=2))\n",
    "plt.colorbar()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "colab_type": "code",
    "id": "B0uVXYL-Kaw4",
    "outputId": "b944d2b7-7549-4c15-a8e3-3a94ab4209e6"
   },
   "outputs": [],
   "source": [
    "platform_types = time_changed_df['Platform_Type'].value_counts()\n",
    "plt.bar(platform_types.index,platform_types)\n",
    "plt.xlim(0, 5)\n",
    "plt.title('Platform Type Frequency')\n",
    "plt.xlabel('Platform_type')\n",
    "plt.ylabel('Number of orders')\n",
    "plt.xticks(np.arange(0, 5, step=1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "colab_type": "code",
    "id": "RdWmVxTbKaw_",
    "outputId": "25fbbc65-3944-44fc-fbe2-b33abb1d5b45"
   },
   "outputs": [],
   "source": [
    "sns.distplot(time_changed_df[\"Pla_Mon\"], kde = False,bins = 4)\n",
    "plt.ylabel('No. of orders')\n",
    "plt.xlabel('Placement day of month')\n",
    "plt.title('Sales per quater')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nQpZbfKZKaxD"
   },
   "source": [
    "Most Orders are placed on the first quarter of the month with most orders happening between hours of 10 AM and 12 PM(last graph and the 2D histogram convey this information), on an hourly basis most orders are recieved on the last 2 days of the month between 10 AM and 12 PM.\n",
    "The majority of customers use Platform_type 3 for their packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "colab_type": "code",
    "id": "3owhHSqIKaxE",
    "outputId": "8f628228-0186-4cef-f295-e4ee575caeb3"
   },
   "outputs": [],
   "source": [
    "# Displays effect of experience on driver delivery time\n",
    "time_changed_df['Rider_Exp']\n",
    "time_changed_df['Time_Pic_Arr']/time_changed_df['Distance(km)']\n",
    "plt.scatter(time_changed_df['Rider_Exp'],\n",
    "            time_changed_df['Time_Pic_Arr']/time_changed_df['Distance(km)'])\n",
    "plt.xticks(np.arange(0, 3, step=1))\n",
    "plt.ylabel('Time diff per km')\n",
    "plt.xlabel('Experience band')\n",
    "plt.title('Effect of experience on delivery time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(time_changed_df['Pla_Time']/3600,\n",
    "            time_changed_df['Time_Pic_Arr']/time_changed_df['Distance(km)'])\n",
    "cdict = {1:'blue',2:'black'}\n",
    "plt.ylabel('Time diff per km')\n",
    "plt.xlabel('Placement_time')\n",
    "plt.title('Effect of placement-time on delivery time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2jXppE5iKaxH"
   },
   "source": [
    "# Zindi Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RTlcB5nJKaxI"
   },
   "outputs": [],
   "source": [
    "lgbm_y = lgbm.predict(test, num_iteration=lgbm.best_iteration)\n",
    "lgbm_output = pd.DataFrame({\"Order No\":test_df['Order No'], \n",
    "                           \"Time from Pickup to Arrival\": lgbm_y })\n",
    "lgbm_output.to_csv(\"submission_2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2_DndruqKaxL"
   },
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e2SIg1iHKaxL"
   },
   "source": [
    "1. A Gentle Introduction to k-fold Cross-Validation\n",
    "by Jason Brownlee on May 23, 2018 in Statistics : https://machinelearningmastery.com/k-fold-cross-validation/\n",
    "\n",
    "2. How to Implement Resampling Methods From Scratch In Python\n",
    "by Jason Brownlee on October 17, 2016 in Code Algorithms From Scratch : https://machinelearningmastery.com/implement-resampling-methods-scratch-python/\n",
    "\n",
    "3. What is the Difference Between Test and Validation Datasets?\n",
    "by Jason Brownlee on July 14, 2017 in Machine Learning Process : https://machinelearningmastery.com/difference-test-validation-datasets/\n",
    "\n",
    "4. ZINDI Discussion Board - Orginal Competition:\n",
    "https://zindi.africa/competitions/sendy-logistics-challenge/discussions"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "Colab_Notebook_Final.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
